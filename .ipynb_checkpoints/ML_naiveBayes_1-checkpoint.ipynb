{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HdbSGxDJLpq"
   },
   "source": [
    "# 1. Vectorizing raw data: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UDtAtXLJLpt"
   },
   "source": [
    "**TF-IDF**\n",
    "\n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9pntkwKNJLpu",
    "outputId": "1d7ed562-1fe4-491a-baa1-e18c2eda45bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# how to upload files in Google drive and load in Colab Notebook\n",
    "from google.colab import drive\n",
    "drive.mount(\"pre_processed_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ZrDYyuqHJhzI",
    "outputId": "76fc3e2c-cfbf-4b6c-fba0-b88b4fc37e99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['cool', 'like', 'would', 'want', 'mother', 'read', 'really', 'great', 'idea', 'well', 'done']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['thank', 'would', 'make', 'life', 'lot', 'le', 'anxietyinducing', 'keep', 'dont', 'let', 'anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['something', 'ill', 'able', 'install', 'site', 'releasing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>['haha', 'guy', 'bunch', 'loser']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                                                              comment_text_lemmatized\n",
       "0  0.000000       ['cool', 'like', 'would', 'want', 'mother', 'read', 'really', 'great', 'idea', 'well', 'done']\n",
       "1  0.000000  ['thank', 'would', 'make', 'life', 'lot', 'le', 'anxietyinducing', 'keep', 'dont', 'let', 'anyon...\n",
       "2  0.000000                                     ['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive']\n",
       "3  0.000000                                         ['something', 'ill', 'able', 'install', 'site', 'releasing']\n",
       "4  0.893617                                                                    ['haha', 'guy', 'bunch', 'loser']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "data = pd.read_csv(\"pre_processed_comments.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "5v7SuAWIJLp5",
    "outputId": "3fe50cc8-a6b3-42b0-8964-92896243d7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 606027)\n"
     ]
    }
   ],
   "source": [
    "# done! don't do it again\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vect.fit_transform(data['comment_text_lemmatized'])\n",
    "print(X_tfidf.shape)\n",
    "#print(tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "FDZzm7HBGf0u",
    "outputId": "e6230f12-97bf-4a96-c24d-aac372dda477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drive/My Drive/Colab Notebooks/tfidf.sav']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# done! don't do it again\n",
    "\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(data['comment_text_lemmatized'])\n",
    "\n",
    "filename = \"drive/My Drive/Colab Notebooks/tfidf.sav\"\n",
    "joblib.dump(tfidf_vect, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "sTOFGlMBG81L",
    "outputId": "ff6d38e4-09cb-4a4a-b0c3-5598301e187d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 606027)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "filename = \"drive/My Drive/Colab Notebooks/tfidf.sav\"\n",
    "tfidf_vect_loaded = joblib.load(filename)\n",
    "X_tfidf = tfidf_vect_loaded.transform(data['comment_text_lemmatized'])\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQdJlLCgJLp-"
   },
   "source": [
    "## What TF-IDF did... (test with small sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkTjKNlKJLqA",
    "outputId": "40677e65-e5e8-4ed4-ffb7-1630394f0d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 63)\n",
      "['able', 'allow', 'animal', 'anxietyinducing', 'anyone', 'bunch', 'combo', 'comment', 'cool', 'design', 'destroy', 'done', 'dont', 'expected', 'ffffuuuuuuuuuuuuuuu', 'get', 'good', 'great', 'greed', 'guy', 'haha', 'hahahahahahahahhha', 'id', 'idea', 'ill', 'impressive', 'install', 'keep', 'kudos', 'land', 'le', 'let', 'life', 'like', 'loser', 'lot', 'make', 'mostly', 'mother', 'motivated', 'one', 'problem', 'public', 'rancher', 'read', 'really', 'releasing', 'right', 'seem', 'show', 'shtty', 'site', 'something', 'suck', 'taking', 'thank', 'together', 'ur', 'urgent', 'want', 'way', 'well', 'would']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>allow</th>\n",
       "      <th>animal</th>\n",
       "      <th>anxietyinducing</th>\n",
       "      <th>anyone</th>\n",
       "      <th>bunch</th>\n",
       "      <th>combo</th>\n",
       "      <th>comment</th>\n",
       "      <th>cool</th>\n",
       "      <th>design</th>\n",
       "      <th>...</th>\n",
       "      <th>suck</th>\n",
       "      <th>taking</th>\n",
       "      <th>thank</th>\n",
       "      <th>together</th>\n",
       "      <th>ur</th>\n",
       "      <th>urgent</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309414</td>\n",
       "      <td>0.263030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.385682</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385682</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       able     allow    animal  anxietyinducing    anyone  bunch     combo  \\\n",
       "0  0.000000  0.000000  0.000000         0.000000  0.000000    0.0  0.000000   \n",
       "1  0.000000  0.000000  0.000000         0.280357  0.280357    0.0  0.000000   \n",
       "2  0.000000  0.000000  0.000000         0.000000  0.000000    0.0  0.000000   \n",
       "3  0.408248  0.000000  0.000000         0.000000  0.000000    0.0  0.000000   \n",
       "4  0.000000  0.000000  0.000000         0.000000  0.000000    0.5  0.000000   \n",
       "5  0.000000  0.000000  0.000000         0.000000  0.000000    0.0  0.000000   \n",
       "6  0.000000  0.000000  0.000000         0.000000  0.000000    0.0  0.000000   \n",
       "7  0.000000  0.000000  0.000000         0.000000  0.000000    0.0  0.000000   \n",
       "8  0.000000  0.288675  0.288675         0.000000  0.000000    0.0  0.000000   \n",
       "9  0.000000  0.000000  0.000000         0.000000  0.000000    0.0  0.385682   \n",
       "\n",
       "   comment      cool    design    ...         suck    taking     thank  \\\n",
       "0  0.00000  0.309414  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1  0.00000  0.000000  0.000000    ...     0.000000  0.000000  0.280357   \n",
       "2  0.00000  0.000000  0.408248    ...     0.000000  0.408248  0.000000   \n",
       "3  0.00000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "4  0.00000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "5  0.57735  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "6  0.00000  0.000000  0.000000    ...     0.707107  0.000000  0.000000   \n",
       "7  0.00000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "8  0.00000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "9  0.00000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "   together       ur    urgent      want       way      well     would  \n",
       "0  0.000000  0.00000  0.000000  0.309414  0.000000  0.309414  0.263030  \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.280357  0.000000  0.238329  \n",
       "2  0.000000  0.00000  0.408248  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.57735  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9  0.385682  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't execute. This is just demonstration\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data_sample = data[0:10]\n",
    "\n",
    "tfidf_vect_sample = TfidfVectorizer()\n",
    "X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['comment_text_lemmatized'])\n",
    "print(X_tfidf_sample.shape)\n",
    "print(tfidf_vect_sample.get_feature_names())\n",
    "\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect_sample.get_feature_names()\n",
    "X_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXi_kTWEJLqH"
   },
   "source": [
    "## 2. Building Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvmduugJKcB2"
   },
   "source": [
    "## Naive Bayes\n",
    "\n",
    "hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEFwtk4aMZLD"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tfidf_dense = X_tfidf.toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_dense, data[\"target\"], test_size=0.2, random_state=123)\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "score = gnb.score(X_test, y_test)\n",
    "print(\"Score: {}\".format(score))\n",
    "\n",
    "filename = \"drive/My Drive/Colab Notebooks/model_NB.sav\"\n",
    "joblib.dump(gnb, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zl6_Iv43RqXI"
   },
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "Y1WF3eV8BGE5",
    "outputId": "6d19df20-66dd-490c-f859-b4f2f1e54a26"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9d6a02689529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, data[\"target\"], test_size=0.2, random_state=123)\n",
    "\n",
    "max_depth = [30, 80, 120]\n",
    "n_estimators = [50, 100, 150]\n",
    "\n",
    "for depth in max_depth:\n",
    "  for n_estimator in  n_estimators:\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.2, max_depth = depth, alpha = 10, n_estimators = n_estimator)\n",
    "\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "    print(\"parameters: colsample_bytree = 0.3, learning_rate = 0.2, max_depth = {}, alpha = 10, n_estimators = {}\".format(depth, n_estimator))\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "    score = xg_reg.score(X_test, y_test)\n",
    "    print(\"Score: {}\".format(score))\n",
    "    \n",
    "    filename = \"drive/My Drive/Colab Notebooks/model_\" + \"depth_\"+ str(depth) + \"n_estimator_\"+ str(n_estimator) +\".sav\"\n",
    "    joblib.dump(xg_reg, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0ybhYXPmtWY"
   },
   "source": [
    "## Real Test"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kQdJlLCgJLp-",
    "Zl6_Iv43RqXI"
   ],
   "name": "ML-naiveBayes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
