{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_ML-naiveBayes_local.ipynb","version":"0.3.2","provenance":[{"file_id":"1FvP96tBG0qPqXbjaTx9Xvd6f-6Vl311u","timestamp":1558638453284},{"file_id":"1pEAkHT1DTFK4lJhNwkRoL5YuRcS7aUc9","timestamp":1558475719419}],"collapsed_sections":["kQdJlLCgJLp-","Zl6_Iv43RqXI"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2HdbSGxDJLpq","colab_type":"text"},"source":["# 1. Vectorizing raw data: TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"_UDtAtXLJLpt","colab_type":"text"},"source":["**TF-IDF**\n","\n","Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document."]},{"cell_type":"code","metadata":{"id":"ZrDYyuqHJhzI","colab_type":"code","outputId":"3a1577b8-8b35-40e8-aca5-e27ed453bc38","executionInfo":{"status":"ok","timestamp":1558643799318,"user_tz":240,"elapsed":10546,"user":{"displayName":"In-Cheol Sun","photoUrl":"https://lh3.googleusercontent.com/-sjS8pLy_w18/AAAAAAAAAAI/AAAAAAAAAKw/SPEANNSMBpY/s64/photo.jpg","userId":"08096968153573468208"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import pandas as pd\n","import numpy\n","pd.set_option('display.max_colwidth', 100)\n","data = pd.read_csv(\"pre_processed_comments_concise.csv\")\n","data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>comment_text_lemmatized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>['cool', 'like', 'would', 'want', 'mother', 'read', 'really', 'great', 'idea', 'well', 'done']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>['thank', 'would', 'make', 'life', 'lot', 'le', 'anxietyinducing', 'keep', 'dont', 'let', 'anyon...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>['something', 'ill', 'able', 'install', 'site', 'releasing']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.893617</td>\n","      <td>['haha', 'guy', 'bunch', 'loser']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     target                                                                              comment_text_lemmatized\n","0  0.000000       ['cool', 'like', 'would', 'want', 'mother', 'read', 'really', 'great', 'idea', 'well', 'done']\n","1  0.000000  ['thank', 'would', 'make', 'life', 'lot', 'le', 'anxietyinducing', 'keep', 'dont', 'let', 'anyon...\n","2  0.000000                                     ['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive']\n","3  0.000000                                         ['something', 'ill', 'able', 'install', 'site', 'releasing']\n","4  0.893617                                                                    ['haha', 'guy', 'bunch', 'loser']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"5v7SuAWIJLp5","colab_type":"code","outputId":"3fe50cc8-a6b3-42b0-8964-92896243d7c9","executionInfo":{"status":"ok","timestamp":1558001124986,"user_tz":240,"elapsed":67155,"user":{"displayName":"In-Cheol Sun","photoUrl":"https://lh3.googleusercontent.com/-sjS8pLy_w18/AAAAAAAAAAI/AAAAAAAAAKw/SPEANNSMBpY/s64/photo.jpg","userId":"08096968153573468208"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# done! don't do it again\n","import joblib\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vect = TfidfVectorizer()\n","X_tfidf = tfidf_vect.fit_transform(data['comment_text_lemmatized'])\n","filename = \"tfidf.sav\"\n","joblib.dump(tfidf_vect, filename)\n","print(X_tfidf.shape)\n","#print(tfidf_vect.get_feature_names())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1804874, 606027)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DXi_kTWEJLqH","colab_type":"text"},"source":["## 2. Building Machine Learning Model"]},{"cell_type":"markdown","metadata":{"id":"MvmduugJKcB2","colab_type":"text"},"source":["## Naive Bayes\n","\n","hyperparameters\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OEFwtk4aMZLD","colab_type":"code","outputId":"861b31d8-8955-4896-8395-bdd037398ec8","executionInfo":{"status":"error","timestamp":1558645545361,"user_tz":240,"elapsed":1564,"user":{"displayName":"In-Cheol Sun","photoUrl":"https://lh3.googleusercontent.com/-sjS8pLy_w18/AAAAAAAAAAI/AAAAAAAAAKw/SPEANNSMBpY/s64/photo.jpg","userId":"08096968153573468208"}},"colab":{"base_uri":"https://localhost:8080/","height":244}},"source":["import joblib\n","import numpy as np\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","X_tfidf_dense = X_tfidf.toarray()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf_dense, data[\"target\"], test_size=0.2, random_state=123)\n","\n","\n","gnb = GaussianNB()\n","gnb.fit(X_train,y_train)\n","y_pred = gnb.predict(X_test)\n","\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE: {}\".format(rmse))\n","\n","score = gnb.score(X_test, y_test)\n","print(\"Score: {}\".format(score))\n","\n","filename = \"model_NB.sav\"\n","joblib.dump(gnb, filename)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-96d01436d075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_tfidf_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tfidf_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_tfidf' is not defined"]}]}]}