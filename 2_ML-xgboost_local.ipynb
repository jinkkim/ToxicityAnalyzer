{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HdbSGxDJLpq"
   },
   "source": [
    "# 1. Vectorizing raw data: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UDtAtXLJLpt"
   },
   "source": [
    "**TF-IDF**\n",
    "\n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10532,
     "status": "ok",
     "timestamp": 1558171326411,
     "user": {
      "displayName": "In-Cheol Sun",
      "photoUrl": "https://lh3.googleusercontent.com/-sjS8pLy_w18/AAAAAAAAAAI/AAAAAAAAAKw/SPEANNSMBpY/s64/photo.jpg",
      "userId": "08096968153573468208"
     },
     "user_tz": 240
    },
    "id": "ZrDYyuqHJhzI",
    "outputId": "c0ee18c7-fbcd-4310-f122-eb90dc258f55"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['cool', 'like', 'would', 'want', 'mother', 'read', 'really', 'great', 'idea', 'well', 'done']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['thank', 'would', 'make', 'life', 'lot', 'le', 'anxietyinducing', 'keep', 'dont', 'let', 'anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>['something', 'ill', 'able', 'install', 'site', 'releasing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>['haha', 'guy', 'bunch', 'loser']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                                                              comment_text_lemmatized\n",
       "0  0.000000       ['cool', 'like', 'would', 'want', 'mother', 'read', 'really', 'great', 'idea', 'well', 'done']\n",
       "1  0.000000  ['thank', 'would', 'make', 'life', 'lot', 'le', 'anxietyinducing', 'keep', 'dont', 'let', 'anyon...\n",
       "2  0.000000                                     ['urgent', 'design', 'problem', 'kudos', 'taking', 'impressive']\n",
       "3  0.000000                                         ['something', 'ill', 'able', 'install', 'site', 'releasing']\n",
       "4  0.893617                                                                    ['haha', 'guy', 'bunch', 'loser']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "data = pd.read_csv(\"pre_processed_comments_concise.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67155,
     "status": "ok",
     "timestamp": 1558001124986,
     "user": {
      "displayName": "In-Cheol Sun",
      "photoUrl": "https://lh3.googleusercontent.com/-sjS8pLy_w18/AAAAAAAAAAI/AAAAAAAAAKw/SPEANNSMBpY/s64/photo.jpg",
      "userId": "08096968153573468208"
     },
     "user_tz": 240
    },
    "id": "5v7SuAWIJLp5",
    "outputId": "3fe50cc8-a6b3-42b0-8964-92896243d7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 606027)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vect.fit_transform(data['comment_text_lemmatized'])\n",
    "\n",
    "filename = \"tfidf.sav\"\n",
    "joblib.dump(tfidf_vect, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXi_kTWEJLqH"
   },
   "source": [
    "## 2. Building Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvmduugJKcB2"
   },
   "source": [
    "## Extreme Gradient Boosting\n",
    "\n",
    "XGBoost's hyperparameters\n",
    "\n",
    "- **learning_rate**: step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "- **max_depth**: determines how deeply each tree is allowed to grow during any boosting round.\n",
    "- **subsample**: percentage of samples used per tree. Low value can lead to underfitting.\n",
    "- **colsample_bytree**: percentage of features used per tree. High value can lead to overfitting.\n",
    "- **n_estimators**: number of trees you want to build.\n",
    "- **objective**: determines the loss function to be used like <br>\n",
    "  &nbsp;&nbsp; *reg:linear* for regression problems<br>\n",
    "  &nbsp;&nbsp; *reg:logistic* for classification problems with only decision <br>\n",
    "  &nbsp;&nbsp; *binary:logistic* for classification problems with probability<br>\n",
    "  \n",
    "  \n",
    " - **gamma**: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n",
    "-**alpha**: L1 regularization on leaf weights. A large value leads to more regularization.\n",
    "-**lambda**: L2 regularization on leaf weights and is smoother than L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmhLTurMMOhG"
   },
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 72747,
     "status": "ok",
     "timestamp": 1557998660685,
     "user": {
      "displayName": "In-Cheol Sun",
      "photoUrl": "https://lh3.googleusercontent.com/-sjS8pLy_w18/AAAAAAAAAAI/AAAAAAAAAKw/SPEANNSMBpY/s64/photo.jpg",
      "userId": "08096968153573468208"
     },
     "user_tz": 240
    },
    "id": "OEFwtk4aMZLD",
    "outputId": "b875d91c-bc1a-4ab0-c84d-baa46416e49b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, data[\"target\"], test_size=0.2, random_state=123)\n",
    "\n",
    "start_time  = datetime.datetime.now()\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.3, max_depth = 120, alpha = 10, n_estimators = 150)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "print(\"parameters: colsample_bytree = 0.3, learning_rate = 0.3, max_depth = 120, alpha = 10, n_estimators = 150\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "score = xg_reg.score(X_test, y_test)\n",
    "print(\"Score: {}%\".format(score))\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print('Select Done..., Time Cost: {}'.format((end_time - start_time).seconds))\n",
    "\n",
    "filename = \"model_\" + \"depth_120\"+\"n_estimator_150\"+\".sav\"\n",
    "joblib.dump(xg_reg, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zl6_Iv43RqXI"
   },
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Y1WF3eV8BGE5",
    "outputId": "b22fa884-5fc5-4cd8-a649-de917264d126"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, data[\"target\"], test_size=0.2, random_state=123)\n",
    "\n",
    "max_depth = [30, 80, 120]\n",
    "n_estimators = [50, 100, 150]\n",
    "\n",
    "for depth in max_depth:\n",
    "  for n_estimator in  n_estimators:\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.2, max_depth = depth, alpha = 10, n_estimators = n_estimator)\n",
    "\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "    print(\"parameters: colsample_bytree = 0.3, learning_rate = 0.2, max_depth = {}, alpha = 10, n_estimators = {}\".format(depth, n_estimator))\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "    score = xg_reg.score(X_test, y_test)\n",
    "    print(\"Score: {}\".format(score))\n",
    "    \n",
    "    filename = \"model_\" + \"depth_\"+ str(depth) + \"n_estimator_\"+ str(n_estimator) +\".sav\"\n",
    "    joblib.dump(xg_reg, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI213MlVsZx8"
   },
   "source": [
    "parameters: colsample_bytree = 0.3, learning_rate = 0.2, max_depth = 30, alpha = 10, n_estimators = 50\n",
    "RMSE: 0.13721690296759934\n",
    "Score: 0.517773667350127\n",
    "\n",
    "parameters: colsample_bytree = 0.3, learning_rate = 0.2, max_depth = 30, alpha = 10, n_estimators = 100\n",
    "RMSE: 0.13398713286524683\n",
    "Score: 0.5402074988799477\n",
    "\n",
    "parameters: colsample_bytree = 0.3, learning_rate = 0.2, max_depth = 30, alpha = 10, n_estimators = 150\n",
    "RMSE: 0.1328326897959838\n",
    "Score: 0.5480965779349566\n",
    "\n",
    "parameters: colsample_bytree = 0.3, learning_rate = 0.2, max_depth = 80, alpha = 10, n_estimators = 50\n",
    "RMSE: 0.13574270172780437\n",
    "Score: 0.5280796842133185\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUKqrGvUR19y"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1, alpha = 10)\n",
    "parameters = {'n_estimators': [50, 100], 'max_depth':[30, 90]}\n",
    "\n",
    "start_time  = datetime.datetime.now()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xg_reg, param_grid=parameters, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_tfidf, data[\"target\"])\n",
    "\n",
    "print(\"Best score: {}\".format(grid_search.best_score_))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "  print(\"\\t{}: {}\".format(param_name, best_parameters[param_name]))\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print('Select Done..., Time Cost: {}'.format((end_time - start_time).seconds))  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2_ML-xgboost_local.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
